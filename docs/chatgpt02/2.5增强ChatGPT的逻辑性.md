# 增强ChatGPT说话的逻辑性

# **1 为何没有逻辑**

众所周知，ChatGPT 回答的**逻辑性不好**。

<img src=".\chatgpt02_imags05\image-20230227141507069.png">

<img src=".\chatgpt02_imags05\image-20230227141517662.png">

正确答案是：7个偶数，3个奇数。ChatGPT甚至连数字都抄错了

然而，我们人类很多时候也是这样，当我们依赖于直觉判断，或是思维跳得太快，没有按部就班一步一步推理时，就很容易犯错，得出错误的答案。不过，当我们遇到一些问题时（如257*37=？），我们懂得不要着急给出答案（就算想也做不到啊），而是会在脑海里先计算推理一番，用「工作记忆」记住计算过程中的临时结果，对于更难的问题，我们会先做一下草稿或利用工具，由此来得到正确的答案。

*注：人脑的「工作记忆」是一种记忆容量有限的认知系统，被用以暂时保存信息。工作记忆对于推理以及指导决策和行为有重要影响。*

**当我们让 ChatGPT 回答问题时，我们并没有给它思考的空间**，它无法利用「工作记忆」，也无法用草稿纸，需要直接给出答案（可能是因为用于训练的数据，少有推理过程）。但如果我们可以让它像人类一样，**把需要多步推理的问题，拆分成子问题**，便可以帮助ChatGPT提高回答的逻辑性，这一点被证明是有效的。

我们可以通过改写 prompt 来达到这个目的，以下将介绍几种提高逻辑性的方法。

1.方法有很多，但并不需要一一记住，**本质都是引导ChatGPT给出中间的推理步骤**，不要直接给结果。

2.随着ChatGPT本身逻辑性的提高，在某些问题上，可以直接得到很好的答案，而不需要使用额外的方法。

3.这些方法源自无对话功能的GPT-3，而ChatGPT可以来回对话，在对话中修正答案，某些时候无需用到这些方法。

# **2 增强逻辑的方法**

## **2.1 思维链(Chain of Thought)**

**CoT可是说是最重要的一种方法**，因此有时候也把其它方法统称为CoT。这个方法利用GPT-3是一个**文本预测器**的特性，它需要尽量保持上下文的连贯性。

CoT 主要分为两种：

1. **Zero-shot-CoT**：在 prompt 的后面加上 Let's think step by step（一步一步地思考）
2. **Chain of Thought**：展示一个相似问题的推理过程，告诉ChatGPT应该这么来（推荐使用👍）。

*区别是前者没有给出推理的示例（zero-shot表示0示例），后者给出至少一个示例。*

我们用这 2 种方法，分别测试一下前面数奇数偶数的问题。

### **Zero-shot-CoT**

在问题的最后加上 Let's think step by step

<img src=".\chatgpt02_imags05\image-20230227141559629.png">

好吧，虽然列出的过程，可最后一步不知为什么失败了🔴。

当然在这个问题中失败了并不意味着以后不能用了，GPT-3并不是一个确定的系统，在某些情况下也许可以得到正确的答案。但这个方法不稳定。

Zero-shot-CoT最大的用处或许是可以为我们的第二种方法 CoT 提供例子。

### **Chain of Thought**

将上一个方法的结果，改写一下，在每一步当中加入当前步数的累计个数，然后给ChatGPT作为处理此类问题的示例。

现在我们得到了正确的答案。🟢

<img src=".\chatgpt02_imags05\image-20230227141717538.png">

同样，尽管把这道题做对了，但改下数字或换个题目，可能就又不行了。

ChatGPT通常都很粗心，**需要把推理的过程需要拆分得很细**，否则很容易犯错，这样的错误可能只是抄错了数字，但一步错便步步错了。

这还只是很简单的问题，便需要写这么复杂的prompt才能解决，甚至还不能保证结果是对的，可见**对待此类问题，我们应当另寻出路**，除非是为了运用费曼学习法，将知识讲于它人听，以加深对知识的理解。

对这个特定的问题：实现一个自动数「奇偶个数」的功能。对大部分人来说，写prompt还是比写代码简单的，但就算如此，ChatGPT的效率是低下的，代码运算只要几毫秒就能得到结果，而ChatGPT要一个字一个字蹦出来，至少要花个十几秒。所以，在ChatGPT能够聪明高效地解决问题前，对于传统的计算问题，还是应该使用传统的方式。

这并不是说 ChatGPT 或 Chain of Thought 派不上用场。**对于需要对语言有所理解才能处理的问题，传统的编程方式难以使用，这是ChatGPT施展手脚的地方。**人类有大部分知识都是用语言记录，有很大的应用空间。

接下来的 2 种方法，是CoT的变种。

## **2.2 提出子问题（Self-Ask）**

有些时候，中间的推理过程更为复杂一些，需要将问题分解成子问题。

Self-Ask 是一种让 GPT-3 根据问题自动提出子问题的方法，要求GPT-3判断一个问题是否提出子问题，先解决子问题后，再给出最后的答案。

<img src=".\chatgpt02_imags05\image-20230227141737037.png">

*我们总是可以用很简单的问题（小学算术）作为示例，然后让ChatGPT处理更难的问题*

<img src=".\chatgpt02_imags05\image-20230227141842528.png">

## **2.3 给出多种方案（Self Consistency)**

要求GPT-3给出一个问题的多个解决方案，最后综合考虑几个方案，得出最终的答案。

<img src=".\chatgpt02_imags05\image-20230227141907329.png">

对于搜索的计算问题，传统的编程有2种算法：深度搜索、广度搜索。如果我们将ChatGPT处理的问题类比到传统的计算问题上，那么Self Consistency可以类比成是广度搜索的组成部分，而Self Ask则是深度搜索的组成部分。

<img src=".\chatgpt02_imags05\image-20230227141935826.png">

该方法已被证明可以提高算术、常识和符号推理任务的结果。即使发现常规 CoT 无效，改方法仍然能够改善结果

除了以上方法外，GPT-3 还能够结合其他工具，例如将问题转成代码，或是从外部获取信息，以此来提高GPT-3回答的准确性，在此不做详细介绍。

# **3 向人类发起挑战**

## **3.1 解决数理问题**

Google利用上面提到的这些方法，做了一个叫Minerva的系统，提高了大语言模型在数理逻辑方面的表现。https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html

> Minerva 结合了最新的 prompt 和评估技术，以更好地解决数学问题。这些包括 Chain of Thought 或 scratchpad （在提出新问题之前，Minerva 会使用几种step-by-step的方法解决现有问题），以及多数投票（也就是Self Consistency）。
> 

*注：scratchpad会将过程中的结果记录下来，和上面数奇偶的方法相似，也可以认为是一种CoT。*

<img src=".\chatgpt02_imags05\image-20230227142029853.png">

Minerva在数理上的表现

当然，正如我们前面提到的，GPT-3在不借助其它工具时，对需要多步推理的问题，处理效率不高，因此解决数理问题，更好的方法可能是和**Wolfram**结合，取GPT-3理解语言之长，补其逻辑推理之短。

## **3.2 机器的直觉**

最后我们来比较一下人类和ChatGPT谁的「直觉」比较厉害。

> 下面是一个相对简单的题目，别费力去分析它，凭直觉做做看：
> 
> 
> **「球拍」和「球」共花 1.10 美元，「球拍」比「球」贵 1 美元，问「球」多少钱？**
> 
> *~~你会马上想到一个数字，这个数字当然就是10，即10美分。这道简单的难题之所以与众不同，是因为它能引出一个直觉性的、吸引人的但却错误的答案。计算一下，你就会发现。如果球花费10美分的话，总共就要花1.20美元（球10美分，球拍1.10美元），而不是1.10美元。正确答案是5美分。~~*
> 
> ——《思考，快与慢》
> 

<img src=".\chatgpt02_imags05\image-20230227142129857.png">

*好吧，这已经不是「直觉」了，ChatGPT开始使用「系统2」了*